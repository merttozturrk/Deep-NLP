{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N5HOT03xLQL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import openpyxl\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.tr import Turkish\n",
        "\n",
        "def cleaner(text):\n",
        "    final_text = ''\n",
        "    for word in text.split():\n",
        "        if word.startswith('@'):\n",
        "            continue\n",
        "        elif word[-3:] in ['com', 'org']:\n",
        "            continue\n",
        "        elif word.startswith(\"RT\"):\n",
        "            continue\n",
        "        elif word.startswith(\"#\"):\n",
        "            continue\n",
        "        elif word.startswith('pic') or word.startswith('http') or word.startswith('www'):\n",
        "            continue\n",
        "        else:\n",
        "            final_text += word+' '\n",
        "    return final_text\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/models/trmodel', binary=True)\n",
        "\n",
        "def feature_extraction(text):\n",
        "    vector = np.zeros(400)\n",
        "    for token in nlp(text):\n",
        "        word = token.text.lower()\n",
        "        if word in word_vectors:\n",
        "            vector+=word_vectors[word]\n",
        "    return vector\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/models/cleaned_2650.xlsx\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqZLDeOExZSM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PGVvEi5yBmf"
      },
      "outputs": [],
      "source": [
        "sentences = df[\"tweet\"].values\n",
        "y = df[\"label\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.15, random_state=1000)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.18, random_state=1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "pI8pmEL9SAhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "w2vec_embeddings = word_vectors.wv.vectors\n",
        "w2vec_vocab = word_vectors.wv.vocab\n",
        "w2vec_vocab = list(w2vec_vocab.keys())"
      ],
      "metadata": {
        "id": "0d5JuF5iS-l_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cf636f-0e5e-47cd-9853-dc26d3a5fb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_index= {}\n",
        "for idx in range(0,len(w2vec_embeddings)):\n",
        "  word = w2vec_vocab[idx]\n",
        "  embedding_index[word] = list(w2vec_embeddings[idx])\n",
        "\n",
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 400\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "id": "cP7kHNErGnhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "import keras\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "DnIkNNu9TlHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(3, activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zWUbf_-PIDE",
        "outputId": "10fd06e0-f467-461f-f5a6-1a94fd9b437f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 400)         3369200   \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, None, 128)         256128    \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPooling  (None, None, 128)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,806,323\n",
            "Trainable params: 437,123\n",
            "Non-trainable params: 3,369,200\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in X_val])).numpy()\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)"
      ],
      "metadata": {
        "id": "KgIIzeh1PWqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_tvXkAQmjC",
        "outputId": "94fcd4f9-9087-4f10-90f3-d03c18b9e06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "15/15 [==============================] - 12s 744ms/step - loss: 1.0361 - acc: 0.4901 - val_loss: 1.0481 - val_acc: 0.4375\n",
            "Epoch 2/3\n",
            "15/15 [==============================] - 11s 727ms/step - loss: 0.9262 - acc: 0.5473 - val_loss: 0.9829 - val_acc: 0.5225\n",
            "Epoch 3/3\n",
            "15/15 [==============================] - 11s 725ms/step - loss: 0.8321 - acc: 0.6176 - val_loss: 0.9855 - val_acc: 0.4900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "wX2ANDWoRu7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = vectorizer(np.array([[s] for s in X_test])).numpy()\n",
        "#predictions = model.predict(x_test)"
      ],
      "metadata": {
        "id": "5dZI0pmYTrJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_x=model.predict(x_test) \n",
        "prediction=np.argmax(predict_x,axis=1)"
      ],
      "metadata": {
        "id": "zF5gHrkaBXYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(metrics.confusion_matrix(y_test, prediction))\n",
        "\n",
        "# Print the precision and recall, among other metrics\n",
        "print(metrics.classification_report(y_test, prediction, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG3UyuV_y6MM",
        "outputId": "3bb5c8f0-7246-46ef-fe11-8815ad6d10f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[84 43  6]\n",
            " [67 89 22]\n",
            " [17 25 39]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.500     0.632     0.558       133\n",
            "           1      0.567     0.500     0.531       178\n",
            "           2      0.582     0.481     0.527        81\n",
            "\n",
            "    accuracy                          0.541       392\n",
            "   macro avg      0.550     0.538     0.539       392\n",
            "weighted avg      0.547     0.541     0.540       392\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mertozturk_W2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}